{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## pre ##"
      ],
      "metadata": {
        "id": "ELo9r1a2x1Vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ebt9fnVg190e"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y2M6MYDp190h"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"当前路径:\", os.getcwd())  # 输出当前工作目录的绝对路径"
      ],
      "metadata": {
        "id": "Jc6u-t0D7L67",
        "outputId": "e6256eb9-9d1d-4a02-e8c6-2c2f2aff40ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前路径: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgVN_Z-k190i"
      },
      "source": [
        "We\\'ll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called `Lang` which has word → index (`word2index`) and index → word\n",
        "(`index2word`) dictionaries, as well as a count of each word\n",
        "`word2count` which will be used to replace rare words later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Wve_jRHI190j"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WK4fzj5190j"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "997gfymu190k"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "\n",
        "# Lowercase, trim, and remove non-letter 非汉字 characters\n",
        "def normalizeString(s):\n",
        "    # print(s)\n",
        "    s = s.lower().strip()\n",
        "    #print(s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fff!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdn25SBV190k"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the `reverse`\n",
        "flag to reverse the pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o207eUX8190k"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    MAX_LENGTH = 0\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = []\n",
        "    # pairs = [[normalizeString(s) for s in (re.split(r'[\\t ]+', l.strip()) + ['', ''])[:2]] for l in lines]\n",
        "    for l in lines:\n",
        "      temp = [normalizeString(s) for s in l.split('\\t')][0:2]\n",
        "      cur_max = max(len(temp[0].split(' ')),len(temp[1].split(' ')))\n",
        "      MAX_LENGTH = max(MAX_LENGTH, cur_max)\n",
        "\n",
        "      pairs.append(temp)\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs,MAX_LENGTH+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PTBbYmne190m",
        "outputId": "270b8874-d734-431e-d571-1404fd34799b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "MAX_LENGTH 35\n",
            "Read 29909 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "chi 27955\n",
            "eng 7001\n",
            "['這些都不是你的錢', 'none of this is your money']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs, MAX_LENGTH = readLangs(lang1, lang2, reverse)\n",
        "    print('MAX_LENGTH',MAX_LENGTH)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    # pairs = filterPairs(pairs)\n",
        "    # print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs, MAX_LENGTH\n",
        "\n",
        "input_lang, output_lang, pairs, MAX_LENGTH = prepareData('eng', 'chi', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FaNggYp190m"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJrkbW3X190m"
      },
      "source": [
        "The Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "olvgIB8a190m"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE8ZNQqN190n"
      },
      "source": [
        "Attention Decoder\n",
        "=================\n",
        "\n",
        "If only the context vector is passed between the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \\\"focus\\\" on a different part of\n",
        "the encoder\\'s outputs for every step of the decoder\\'s own outputs.\n",
        "First we calculate a set of *attention weights*. These will be\n",
        "multiplied by the encoder output vectors to create a weighted\n",
        "combination. The result (called `attn_applied` in the code) should\n",
        "contain information about that specific part of the input sequence, and\n",
        "thus help the decoder choose the right output words.\n",
        "\n",
        "![](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer `attn`, using the decoder\\'s input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/attention-decoder-network.png)\n",
        "\n",
        "Bahdanau attention, also known as additive attention, is a commonly used\n",
        "attention mechanism in sequence-to-sequence models, particularly in\n",
        "neural machine translation tasks. It was introduced by Bahdanau et al.\n",
        "in their paper titled [Neural Machine Translation by Jointly Learning to\n",
        "Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). This\n",
        "attention mechanism employs a learned alignment model to compute\n",
        "attention scores between the encoder and decoder hidden states. It\n",
        "utilizes a feed-forward neural network to calculate alignment scores.\n",
        "\n",
        "However, there are alternative attention mechanisms available, such as\n",
        "Luong attention, which computes attention scores by taking the dot\n",
        "product between the decoder hidden state and the encoder hidden states.\n",
        "It does not involve the non-linear transformation used in Bahdanau\n",
        "attention.\n",
        "\n",
        "In this tutorial, we will be using Bahdanau attention. However, it would\n",
        "be a valuable exercise to explore modifying the attention mechanism to\n",
        "use Luong attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mXsFSyXq190n"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAX LEN\",MAX_LENGTH)"
      ],
      "metadata": {
        "id": "Jd286Ofz1Km-",
        "outputId": "50e28a04-004b-4a59-ea1f-0538359c7b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX LEN 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaub1aaU190n"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>There are other forms of attention that work around the lengthlimitation by using a relative position approach. Read about \"localattention\" in <a href=\"https://arxiv.org/abs/1508.04025\">Effective Approaches to Attention-based Neural MachineTranslation</a>.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1-C0bebD190n"
      },
      "outputs": [],
      "source": [
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    input_lang, output_lang, pairs, MAX_LENGTH = prepareData('eng', 'chi', True)\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    # 创建TensorDataset\n",
        "    full_dataset = TensorDataset(\n",
        "        torch.LongTensor(input_ids).to(device),\n",
        "        torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    # 按9:1划分训练集和测试集\n",
        "    train_size = int(0.2 * n)\n",
        "    test_size = n - train_size\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "    print(\"train data size\",train_size,'\\n',\"test data size\",test_size)\n",
        "    # 创建训练集和测试集DataLoader\n",
        "    train_sampler = RandomSampler(train_dataset)  # 训练集随机采样\n",
        "    test_sampler = SequentialSampler(test_dataset) # 测试集顺序采样\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=train_sampler,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        sampler=test_sampler,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    return input_lang, output_lang, train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(batch_size ):\n",
        "    test_ratio=0.5\n",
        "    # 固定随机种子\n",
        "    seed=42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # 加载数据\n",
        "    input_lang, output_lang, pairs, MAX_LENGTH = prepareData('eng', 'chi', True)\n",
        "\n",
        "    # 直接划分原始 pairs（保留测试集原始格式）\n",
        "    n = len(pairs)\n",
        "    indices = list(range(n))\n",
        "    random.shuffle(indices)  # 随机打乱索引\n",
        "\n",
        "    # 按比例划分训练集和测试集\n",
        "    split = int(n * (1 - test_ratio))\n",
        "    train_indices = indices[:split]\n",
        "    test_indices = indices[split:]\n",
        "    print(\"train data size\",split,'\\n',\"test data size\",n-split)\n",
        "    train_pairs = [pairs[i] for i in train_indices]\n",
        "    test_pairs = [pairs[i] for i in test_indices]  # 测试集保持原始格式\n",
        "\n",
        "    # 仅处理训练集的张量转换\n",
        "    n_train = len(train_pairs)\n",
        "    input_ids = np.zeros((n_train, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n_train, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(train_pairs):\n",
        "      inp_ids = indexesFromSentence(input_lang, inp)\n",
        "      tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "      inp_ids.append(EOS_token)\n",
        "      tgt_ids.append(EOS_token)\n",
        "      input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "      target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    # 创建训练集 DataLoader\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.LongTensor(input_ids).to(device),\n",
        "        torch.LongTensor(target_ids).to(device)\n",
        "    )\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=RandomSampler(train_dataset),\n",
        "        batch_size=batch_size,\n",
        "        drop_last=True  # 丢弃最后一个不完整批次\n",
        "    )\n",
        "\n",
        "    # 测试集保持原始 pairs 格式，不转换为张量\n",
        "    return input_lang, output_lang, train_dataloader, train_pairs, test_pairs"
      ],
      "metadata": {
        "id": "n49F4Zjl9ax4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzRyDwG_190o"
      },
      "source": [
        "Training the Model\n",
        "==================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-OIuK2PG190o"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PTxbL0OR190o"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pVRsq6Kb190o"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-ZMJkuX190o"
      },
      "source": [
        "Plotting results\n",
        "================\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "`plot_losses` saved while training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nNAmNbyq190o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBa3XsrU190o"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder\\'s predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder\\'s\n",
        "attention outputs for display later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Vy99ZQs7190o"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKb4evxj190o"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LrUfPF5T190o"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    seed=42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    print(\"begin randomly evaluting 10 train data\")\n",
        "    evaluate_inputs = []\n",
        "    for i in range(n):\n",
        "        pair = random.choice(train_pairs)\n",
        "        evaluate_inputs.append(pair)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "    print(\"begin randomly evaluting 10 test data\")\n",
        "    for i in range(n):\n",
        "        pair = random.choice(test_pairs)\n",
        "        evaluate_inputs.append(pair)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "    return evaluate_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06rgyaW8190o"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we\\'ll get\n",
        "some reasonable results.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>If you run this notebook you can train, interrupt the kernel,evaluate, and continue training later. Comment out the lines where theencoder and decoder are initialized and run <code>trainIters</code> again.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "first_batch = next(iter(train_dataloader))\n",
        "for data in train_dataloader:\n",
        "  if data[0].size(0) != 32 or data[1].size(0) != 32 or data[0].size(1) != 35 or data[1].size(1) != 35:\n",
        "    print(data[0].size(),data[1].size())\n"
      ],
      "metadata": {
        "id": "st6gSHeGdtXd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "m8IOIzUU190s",
        "outputId": "1c6e8369-4d9c-4782-fae3-9ea218ef38ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "MAX_LENGTH 35\n",
            "Read 29909 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "chi 27955\n",
            "eng 7001\n",
            "train data size 14954 \n",
            " test data size 14955\n",
            "0m 33s (- 33m 2s) (1 1%) 1.3331\n",
            "1m 6s (- 32m 10s) (2 3%) 0.9979\n",
            "1m 40s (- 31m 41s) (3 5%) 0.9200\n",
            "2m 13s (- 31m 10s) (4 6%) 0.8679\n",
            "2m 47s (- 30m 38s) (5 8%) 0.8257\n",
            "3m 20s (- 30m 4s) (6 10%) 0.7879\n",
            "3m 53s (- 29m 31s) (7 11%) 0.7542\n",
            "4m 27s (- 28m 55s) (8 13%) 0.7228\n",
            "5m 0s (- 28m 23s) (9 15%) 0.6932\n",
            "5m 34s (- 27m 50s) (10 16%) 0.6661\n",
            "6m 7s (- 27m 16s) (11 18%) 0.6405\n",
            "6m 40s (- 26m 43s) (12 20%) 0.6153\n",
            "7m 14s (- 26m 10s) (13 21%) 0.5925\n",
            "7m 47s (- 25m 35s) (14 23%) 0.5702\n",
            "8m 20s (- 25m 2s) (15 25%) 0.5487\n",
            "8m 54s (- 24m 29s) (16 26%) 0.5291\n",
            "9m 27s (- 23m 56s) (17 28%) 0.5100\n",
            "10m 1s (- 23m 22s) (18 30%) 0.4919\n",
            "10m 34s (- 22m 49s) (19 31%) 0.4741\n",
            "11m 7s (- 22m 15s) (20 33%) 0.4570\n",
            "11m 41s (- 21m 42s) (21 35%) 0.4414\n",
            "12m 14s (- 21m 8s) (22 36%) 0.4263\n",
            "12m 47s (- 20m 35s) (23 38%) 0.4119\n",
            "13m 21s (- 20m 2s) (24 40%) 0.3972\n",
            "13m 54s (- 19m 28s) (25 41%) 0.3844\n",
            "14m 27s (- 18m 55s) (26 43%) 0.3719\n",
            "15m 1s (- 18m 22s) (27 45%) 0.3596\n",
            "15m 35s (- 17m 49s) (28 46%) 0.3478\n",
            "16m 8s (- 17m 15s) (29 48%) 0.3360\n",
            "16m 42s (- 16m 42s) (30 50%) 0.3245\n",
            "17m 15s (- 16m 8s) (31 51%) 0.3137\n",
            "17m 48s (- 15m 35s) (32 53%) 0.3024\n",
            "18m 22s (- 15m 1s) (33 55%) 0.2938\n",
            "18m 55s (- 14m 28s) (34 56%) 0.2842\n",
            "19m 29s (- 13m 55s) (35 58%) 0.2752\n",
            "20m 2s (- 13m 21s) (36 60%) 0.2665\n",
            "20m 36s (- 12m 48s) (37 61%) 0.2578\n",
            "21m 9s (- 12m 14s) (38 63%) 0.2499\n",
            "21m 42s (- 11m 41s) (39 65%) 0.2414\n",
            "22m 16s (- 11m 8s) (40 66%) 0.2337\n",
            "22m 49s (- 10m 34s) (41 68%) 0.2274\n",
            "23m 23s (- 10m 1s) (42 70%) 0.2206\n",
            "23m 56s (- 9m 27s) (43 71%) 0.2128\n",
            "24m 29s (- 8m 54s) (44 73%) 0.2069\n",
            "25m 3s (- 8m 21s) (45 75%) 0.1993\n",
            "25m 36s (- 7m 47s) (46 76%) 0.1952\n",
            "26m 9s (- 7m 14s) (47 78%) 0.1888\n",
            "26m 43s (- 6m 40s) (48 80%) 0.1823\n",
            "27m 15s (- 6m 7s) (49 81%) 0.1767\n",
            "27m 49s (- 5m 33s) (50 83%) 0.1729\n",
            "28m 22s (- 5m 0s) (51 85%) 0.1666\n",
            "28m 56s (- 4m 27s) (52 86%) 0.1625\n",
            "29m 29s (- 3m 53s) (53 88%) 0.1577\n",
            "30m 2s (- 3m 20s) (54 90%) 0.1538\n",
            "30m 35s (- 2m 46s) (55 91%) 0.1497\n",
            "31m 8s (- 2m 13s) (56 93%) 0.1444\n",
            "31m 42s (- 1m 40s) (57 95%) 0.1420\n",
            "32m 15s (- 1m 6s) (58 96%) 0.1371\n",
            "32m 49s (- 0m 33s) (59 98%) 0.1329\n",
            "33m 22s (- 0m 0s) (60 100%) 0.1287\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader, train_pairs, test_pairs = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 60, print_every=1, plot_every=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dR6P49-190s"
      },
      "source": [
        "Set dropout layers to `eval` mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "aXQTrmOu190s",
        "outputId": "2c018ade-b7a8-43b8-afd0-bde813b48b57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin randomly evaluting 5 train data\n",
            "> 我生病了\n",
            "= i ve been sick\n",
            "< i am answer <EOS>\n",
            "\n",
            "> 他闖入一間房子\n",
            "= he broke into a house\n",
            "< <EOS>\n",
            "\n",
            "> 谁也不知道他是否爱她\n",
            "= no one knows if he loves her or not\n",
            "< he goes to count <EOS>\n",
            "\n",
            "> 她什么时候结婚的\n",
            "= when did she get married ?\n",
            "< quickly gets toothaches side yesterday <EOS>\n",
            "\n",
            "> 他以唱歌為生\n",
            "= he makes his living by singing\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> 一輛獨輪車有一個輪子\n",
            "= a unicycle has one wheel\n",
            "< a truck kicked ticket air a truck still bicycle worn asleep ticket one else has changed three years still vase vase <EOS>\n",
            "\n",
            "> 他這個人很難相處\n",
            "= he is difficult to get along with\n",
            "< he is crazy <EOS>\n",
            "\n",
            "> 半途而废是你所能做的最糟糕的事情\n",
            "= leaving something unfinished is the worst thing you can do\n",
            "< tomorrow s name of business is business <EOS>\n",
            "\n",
            "> 我不記得哪個是我的球拍\n",
            "= i can t remember which is my racket\n",
            "< i should have bought is <EOS>\n",
            "\n",
            "> 我需要他的帮助\n",
            "= i need his help\n",
            "< i couldn t help me <EOS>\n",
            "\n",
            "begin randomly evaluting 5 test data\n",
            "> 湯姆什麼時候被開除的 ?\n",
            "= when was tom fired ?\n",
            "< you ve been beautiful restaurant near grab SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> 她穿著漂亮的洋裝\n",
            "= she wore a beautiful dress\n",
            "< i m studying french <EOS>\n",
            "\n",
            "> 汤姆正在教我怎么开帆船\n",
            "= tom is teaching me to how to sail\n",
            "< are you happy <EOS>\n",
            "\n",
            "> 完全按您想的来\n",
            "= it will be done just as you wish\n",
            "< is she liked tennis of these people worry <EOS>\n",
            "\n",
            "> 医生把他叫了回来\n",
            "= the doctor called him back\n",
            "< SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> 他喝了一整瓶的牛奶\n",
            "= he drank a whole bottle of milk\n",
            "< is tom thought that mary offered like mary hadn never want to paint SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS SOS\n",
            "\n",
            "> 你知道怎麼煮肉嗎 ?\n",
            "= do you know how to cook meat ?\n",
            "< would you have tea ? <EOS>\n",
            "\n",
            "> 明天九点打电话给我吧\n",
            "= phone me tomorrow at nine\n",
            "< i m studying in critical condition <EOS>\n",
            "\n",
            "> 下周工厂可能会关\n",
            "= possibly the factory will be closed down next week\n",
            "< <EOS>\n",
            "\n",
            "> 湯姆問瑪麗她把鑰匙放在哪裡了\n",
            "= tom asked mary where she d put the key\n",
            "< is white related clothes ? <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluate_inputs = evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bob5xTqS190t"
      },
      "source": [
        "Visualizing Attention\n",
        "=====================\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run `plt.matshow(attentions)` to see attention output\n",
        "displayed as a matrix. For a better viewing experience we will do the\n",
        "extra work of adding axes and labels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ZA2aSV190t"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "evaluate_inputs\n",
        "for pair in evaluate_inputs:\n",
        "\n",
        "  evaluateAndShowAttention(pair[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCsumzaq190t"
      },
      "source": [
        "Exercises\n",
        "=========\n",
        "\n",
        "-   Try with a different dataset\n",
        "    -   Another language pair\n",
        "    -   Human → Machine (e.g. IOT commands)\n",
        "    -   Chat → Response\n",
        "    -   Question → Answer\n",
        "-   Replace the embeddings with pretrained word embeddings such as\n",
        "    `word2vec` or `GloVe`\n",
        "-   Try with more layers, more hidden units, and more sentences. Compare\n",
        "    the training time and results.\n",
        "-   If you use a translation file where pairs have two of the same\n",
        "    phrase (`I am test \\t I am test`), you can use this as an\n",
        "    autoencoder. Try this:\n",
        "    -   Train as an autoencoder\n",
        "    -   Save only the Encoder network\n",
        "    -   Train a new Decoder for translation from there\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}