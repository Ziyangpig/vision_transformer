{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Code Example - Part A\n",
    "\n",
    "This code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "This code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data and pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import some necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    " \n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 128\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"Adam\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")\n",
    "\n",
    "# preprocessing pipeline for input images\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    transformation[data_type] = tv_transforms.Compose(([\n",
    "        tv_transforms.RandomRotation(degrees=15),\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:25<00:00, 1.99MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# prepare datasets\n",
    "\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        \n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, stride=1, padding=0, Is_BN=True, Is_reg=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.Is_reg = Is_reg\n",
    "        self.Is_BN = Is_BN\n",
    "        if Is_BN:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        if Is_reg:\n",
    "            self.maxpool = nn.MaxPool2d(2)\n",
    "            self.dp = nn.Dropout(0.3)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(f\"[DEBUG] Input shape: {x.shape if x is not None else 'NULL'}\")\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        if self.Is_BN:\n",
    "            x = self.bn(x)\n",
    "        if self.Is_reg:\n",
    "            x = self.maxpool(x)\n",
    "            x = self.dp(x)\n",
    "        # print(f\"[DEBUG] Post-conv shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class ConvBs(nn.Module):\n",
    "    def __init__(self, num_layers,layer_dict):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConvBlock(**layer_dict[i])\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 12.96M\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.28M\n"
     ]
    }
   ],
   "source": [
    "# our network architecture\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模块化设计\n",
    "# 增加filter 和layers\n",
    "# google net : v1:NIN+global pooling\n",
    "# v2: BN + 5*5 -> 2 3*3\n",
    "# v3: factorization\n",
    "# residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, normal_\n",
    "\n",
    "class AddPositionEmbs(nn.Module):\n",
    "    def __init__(self, seq_len, emb_dim):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, emb_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "    def __init__(self, in_dim, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 初始化参数\n",
    "        xavier_uniform_(self.fc1.weight)\n",
    "        normal_(self.fc1.bias, std=1e-6)\n",
    "        xavier_uniform_(self.fc2.weight)\n",
    "        normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Encoder1DBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, mlp_dim, num_heads, dropout=0.1, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = MlpBlock(hidden_dim, mlp_dim, dropout)\n",
    "        \n",
    "        # 注意力层初始化\n",
    "        xavier_uniform_(self.attn.in_proj_weight)\n",
    "        normal_(self.attn.in_proj_bias, std=1e-6)\n",
    "        xavier_uniform_(self.attn.out_proj.weight)\n",
    "        normal_(self.attn.out_proj.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(\n",
    "            query=self.norm1(x),\n",
    "            key=self.norm1(x),\n",
    "            value=self.norm1(x)\n",
    "        )\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_dim, mlp_dim, num_heads, dropout=0.1, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Encoder1DBlock(hidden_dim, mlp_dim, num_heads, dropout, attn_dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pos_emb = AddPositionEmbs(seq_len=65, emb_dim=hidden_dim)  # 默认ViT-B/16\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, train=True):\n",
    "        x = self.pos_emb(x)\n",
    "        x = self.dropout(x) if train else x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes, \n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 hidden_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_dim=3072,\n",
    "                 dropout=0.1,\n",
    "                 attn_dropout=0.1,\n",
    "                 representation_size=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "        # 分类token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        \n",
    "        # Transformer编码器\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            hidden_dim=hidden_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            attn_dropout=attn_dropout\n",
    "        )\n",
    "        \n",
    "        # 分类头\n",
    "        self.pre_logits = nn.Identity()\n",
    "        if representation_size:\n",
    "            self.pre_logits = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, representation_size),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            hidden_dim = representation_size\n",
    "            \n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # 初始化\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 卷积层初始化\n",
    "        nn.init.xavier_uniform_(self.patch_embed.weight)\n",
    "        nn.init.normal_(self.patch_embed.bias, std=1e-6)\n",
    "        \n",
    "        # 分类头初始化\n",
    "        nn.init.zeros_(self.head.weight)\n",
    "        nn.init.constant_(self.head.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 分块嵌入 [B, C, H, W] -> [B, hidden_dim, grid, grid]\n",
    "        x = self.patch_embed(x)  \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # 展平并转置 [B, C, H*W] -> [B, H*W, C]\n",
    "        x = x.flatten(2).transpose(1, 2)  \n",
    "        \n",
    "        # 添加分类token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Transformer编码\n",
    "        x = self.encoder(x, self.training)\n",
    "        \n",
    "        # 分类\n",
    "        x = x[:, 0]  # 取分类token\n",
    "        x = self.pre_logits(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(net,file_name):\n",
    "    # the network optimizer\n",
    "    optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training loop\n",
    "    # training loop\n",
    "    net.train()\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_t = 0\n",
    "        running_loss = 0.0\n",
    "        for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "            s = time()\n",
    "            img, target = img.to(device), target.to(device)\n",
    "\n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            e = time()\n",
    "            epoch_t += e - s\n",
    "            if i ==0:\n",
    "                output = f\"time: {e-s:.3f} seconds\"\n",
    "                print(output)\n",
    "            \n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % print_every == print_every - 1:\n",
    "                output = f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f} epoch time: {epoch_t:.3f} seconds\"\n",
    "                print(output)\n",
    "                outputs.append(output)\n",
    "\n",
    "                outputs.append(output)\n",
    "                with open(file_name, \"w\") as f:\n",
    "                    for out in outputs:\n",
    "                        f.write(out + \"\\n\")\n",
    "                running_loss = 0.0\n",
    "                epoch_t = 0\n",
    "    print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net):\n",
    "    net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            # make prediction\n",
    "            pred = net(img)\n",
    "            \n",
    "            # accumulate\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "    print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.644 seconds\n",
      "[epoch=  1, iter=  200] loss: 2.004 epoch time: 105.239 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m VisionTransformer(\n\u001b[0;32m      2\u001b[0m                         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m                             attn_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     11\u001b[0m                             representation_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m train(net,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVit_output.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m evaluate_accuracy(net)\n",
      "Cell \u001b[1;32mIn[39], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, file_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m e \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = VisionTransformer(\n",
    "                        num_classes=10,\n",
    "                            img_size=32,\n",
    "                            patch_size=4,\n",
    "                            hidden_dim=192,\n",
    "                            num_layers=6,\n",
    "                            num_heads=6,\n",
    "                            mlp_dim=768,\n",
    "                            dropout=0.1,\n",
    "                            attn_dropout=0.1,\n",
    "                            representation_size=None)\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n",
    "train(net,\"Vit_output.txt\")\n",
    "evaluate_accuracy(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 12.96M\n",
      "time: 10.272 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# print the number of parameters\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mnet\u001b[38;5;241m.\u001b[39mparameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrequires_grad)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1_000_000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m train(net,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBN_output.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m evaluate_accuracy(net)\n",
      "Cell \u001b[1;32mIn[39], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, file_name)\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m e \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32md:\\software\\Anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dict = {\n",
    "    0: {'in_channels': 3, 'out_channels': 128, 'kernel_size': 3, 'Is_reg':False, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    1: {'in_channels': 128, 'out_channels': 256, 'kernel_size': 3, 'Is_reg': False, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    2: {'in_channels': 256, 'out_channels': 512, 'kernel_size': 3, 'Is_reg': False, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    3: {'in_channels': 512, 'out_channels': 1024, 'kernel_size': 3, 'Is_reg': False, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    4: {'in_channels': 1024, 'out_channels': 512, 'kernel_size': 3, 'Is_reg': True, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    5: {'in_channels': 512, 'out_channels': 256, 'kernel_size': 3, 'Is_reg': True, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "    6: {'in_channels': 256, 'out_channels': 128, 'kernel_size': 3, 'Is_reg': True, 'Is_BN': True, 'stride': 1, 'padding': 1, },\n",
    "}\n",
    "net  =nn.Sequential(\n",
    "ConvBs(7, layer_dict),\n",
    "nn.Flatten(),\n",
    "nn.Linear(128 * 4 * 4, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "nn.Linear(128, 10),\n",
    ")\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n",
    "train(net,\"BN_output.txt\")\n",
    "evaluate_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our network architecture\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n",
    "train(net,\"init_output.txt\")\n",
    "evaluate_accuracy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
